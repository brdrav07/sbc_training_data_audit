
#███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗
#██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║
#╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝
#                                                                                                                                                                  
#                  ███████╗██████╗  ██████╗    ████████╗██████╗  █████╗ ██╗███╗   ██╗██╗███╗   ██╗ ██████╗     ██████╗  █████╗ ████████╗ █████╗                    
#                  ██╔════╝██╔══██╗██╔════╝    ╚══██╔══╝██╔══██╗██╔══██╗██║████╗  ██║██║████╗  ██║██╔════╝     ██╔══██╗██╔══██╗╚══██╔══╝██╔══██╗                   
#                  ███████╗██████╔╝██║            ██║   ██████╔╝███████║██║██╔██╗ ██║██║██╔██╗ ██║██║  ███╗    ██║  ██║███████║   ██║   ███████║                   
#                  ╚════██║██╔══██╗██║            ██║   ██╔══██╗██╔══██║██║██║╚██╗██║██║██║╚██╗██║██║   ██║    ██║  ██║██╔══██║   ██║   ██╔══██║                   
#                  ███████║██████╔╝╚██████╗       ██║   ██║  ██║██║  ██║██║██║ ╚████║██║██║ ╚████║╚██████╔╝    ██████╔╝██║  ██║   ██║   ██║  ██║                   
#                  ╚══════╝╚═════╝  ╚═════╝       ╚═╝   ╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝╚═╝╚═╝  ╚═══╝ ╚═════╝     ╚═════╝ ╚═╝  ╚═╝   ╚═╝   ╚═╝  ╚═╝        
#                                                                                                                                                                  
#███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗███╗
#██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║██╔╝╚██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║██║  ██║
#███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║███╗███║
#╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝╚══╝

# Authored by Brodie Verrall
# Last updated: 2025-08-01

#$$$ This script generates Spatial BioCondition training datasets by generating a pool of sites from various sources, scoring them against benchmarks, and then
#$$$ assesses each site against a series of auditing criteria. Spatial datasets are available via QSpatial or TERN data portal.This workflow has several parts:

# 1) Workspace setup: creates the project directory structure and loads required libraries
# 2) Site summaries: imports and processes raw tabular data from QBEIS and QBERD to generate site summaries of BioCondition attributes
# 3) Site compiler: compiles all site summaries into a single training data pool with standardised formatting and data structure
# 4) RE assignment: checks site REs against the latest version of REDD, and changes/assigns conflicts via RE mapping, landzone  and composition data
# 5) Site scoring: scores sites against the latest version of benchmarks (including drafts), and assigns a score to each site where possible
# 6) Site assessment: assesses sites against a series of spatial criteria to ensure they meet the requirements for inclusion in the training dataset
# 7) Site auditing: selects sites based on the scoring and assessment criteria, and generates a final training dataset

#TODO: ensure all inputs are current and stored in the relevant input folder in /project_data

##### 1) Workspace setup #####
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#           ██╗       ██╗    ██╗ ██████╗ ██████╗ ██╗  ██╗███████╗██████╗  █████╗  ██████╗███████╗    ███████╗████████╗███████╗██╗   ██╗██████╗ 
#          ███║       ██║    ██║██╔═══██╗██╔══██╗██║ ██╔╝██╔════╝██╔══██╗██╔══██╗██╔════╝██╔════╝    ██╔════╝╚══██╔══╝██╔════╝██║   ██║██╔══██╗
#          ╚██║       ██║ █╗ ██║██║   ██║██████╔╝█████╔╝ ███████╗██████╔╝███████║██║     █████╗      ███████╗   ██║   █████╗  ██║   ██║██████╔╝
#           ██║       ██║███╗██║██║   ██║██╔══██╗██╔═██╗ ╚════██║██╔═══╝ ██╔══██║██║     ██╔══╝      ╚════██║   ██║   ██╔══╝  ██║   ██║██╔═══╝ 
#           ██║██╗    ╚███╔███╔╝╚██████╔╝██║  ██║██║  ██╗███████║██║     ██║  ██║╚██████╗███████╗    ███████║   ██║   ███████╗╚██████╔╝██║     
#           ╚═╝╚═╝     ╚══╝╚══╝  ╚═════╝ ╚═╝  ╚═╝╚═╝  ╚═╝╚══════╝╚═╝     ╚═╝  ╚═╝ ╚═════╝╚══════╝    ╚══════╝   ╚═╝   ╚══════╝ ╚═════╝ ╚═╝     
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝  

# 1.1) Check renv status and clean workspace (optional)
# renv::status()
rm(list = ls())
gc()

# 1.2) Install and load project library
packages <- c(
  "tidyverse", 
  "arrow", 
  "purrr", 
  "furrr", 
  "fs", 
  "here", 
  "jsonlite", 
  "sf", 
  "lwgeom", 
  "terra"
)

installed_packages <- rownames(installed.packages())
for (pkg in packages) {
  if (!pkg %in% installed_packages) {
    install.packages(pkg)
  }
}

lapply(packages, library, character.only = TRUE)
rm(installed_packages, packages, pkg)
gc()

# 1.3) clean workspace, set up directories and wd
proj_folders <- c("archive", "project_data", "scripts")
invisible(lapply(proj_folders, function(folder) {
  path <- here(folder)
  if (!dir.exists(path)) {
    dir.create(path)
    message(sprintf("'%s' folder created.", folder))
  } else {
    message(sprintf("'%s' folder already exists.", folder))
  }
}))
setwd(here("project_data"))
message("Working directory set to: ", getwd())

subfolders <- c("training_data_inputs", "spatial_inputs", "tabular_inputs", "intermediates", "outputs")
invisible(lapply(subfolders, function(folder) {
  if (!dir.exists(folder)) {
    dir.create(folder)
    message(sprintf("'%s' folder created.", folder))
  } else {
    message(sprintf("'%s' folder already exists.", folder))
  }
}))
int_dir <-file.path("intermediates/")
output_dir <- file.path("outputs/")

# 1.5) Import other required tabular data
redd <- read.csv("tabular_inputs/REDD_v13.1_2024.csv")
re <- read.csv("tabular_inputs/regional_ecosystem_2024.csv")
deficit <- read.csv("tabular_inputs/SBC_td_deficit_REv14.csv")

### TODO: Intergrate spatial data in each section to pipe in and kick out only when needed
# SBC_td_v7 <-  sf::st_read("spatial_inputs/SBC_TDv7.5_WOS_BVG_STRUCTURE.gpkg", quiet = TRUE) # last version of WOS td
# pre_clear <- sf::st_read("spatial_inputs/Preclear_v13-1_2021/data.gdb", quiet = TRUE) # pre-clearance RE mapping
# remnant <- sf::st_read("spatial_inputs/Remnant_v13-1_2021/data.gdb", quiet = TRUE) # remnant vegetation mapping
# hvr <- sf::st_read("spatial_inputs/HVR_v13-1-1_2021/data.gdb", quiet = TRUE) # high value regrowth mapping
# slats_clearing <- sf::st_read("spatial_inputs/SLATS_2023/SLATS_9195/data.gdb", quiet = TRUE) #slats clearing 
# sentinel_grid <-





##### 2) Site summaries #####
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                    ██████╗        ███████╗██╗████████╗███████╗    ███████╗██╗   ██╗███╗   ███╗███╗   ███╗ █████╗ ██████╗ ██╗███████╗███████╗               
#                    ╚════██╗       ██╔════╝██║╚══██╔══╝██╔════╝    ██╔════╝██║   ██║████╗ ████║████╗ ████║██╔══██╗██╔══██╗██║██╔════╝██╔════╝               
#                     █████╔╝       ███████╗██║   ██║   █████╗      ███████╗██║   ██║██╔████╔██║██╔████╔██║███████║██████╔╝██║█████╗  ███████╗               
#                    ██╔═══╝        ╚════██║██║   ██║   ██╔══╝      ╚════██║██║   ██║██║╚██╔╝██║██║╚██╔╝██║██╔══██║██╔══██╗██║██╔══╝  ╚════██║               
#                    ███████╗██╗    ███████║██║   ██║   ███████╗    ███████║╚██████╔╝██║ ╚═╝ ██║██║ ╚═╝ ██║██║  ██║██║  ██║██║███████╗███████║               
#                    ╚══════╝╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚══════╝ ╚═════╝ ╚═╝     ╚═╝╚═╝     ╚═╝╚═╝  ╚═╝╚═╝  ╚═╝╚═╝╚══════╝╚══════╝               
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

# TODO: Pipeline in QBERD and QBEIS site summary code here, and ensure extra columns are included that include landzone, dominant floristics, composition, disturbance notes etc



##### 3) Site complier #####                                                                                                                                                           
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                        ██████╗        ███████╗██╗████████╗███████╗     ██████╗ ██████╗ ███╗   ███╗██████╗ ██╗██╗     ███████╗██████╗                           
#                        ╚════██╗       ██╔════╝██║╚══██╔══╝██╔════╝    ██╔════╝██╔═══██╗████╗ ████║██╔══██╗██║██║     ██╔════╝██╔══██╗                          
#                         █████╔╝       ███████╗██║   ██║   █████╗      ██║     ██║   ██║██╔████╔██║██████╔╝██║██║     █████╗  ██████╔╝                          
#                         ╚═══██╗       ╚════██║██║   ██║   ██╔══╝      ██║     ██║   ██║██║╚██╔╝██║██╔═══╝ ██║██║     ██╔══╝  ██╔══██╗                          
#                        ██████╔╝██╗    ███████║██║   ██║   ███████╗    ╚██████╗╚██████╔╝██║ ╚═╝ ██║██║     ██║███████╗███████╗██║  ██║                          
#                        ╚═════╝ ╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝     ╚═════╝ ╚═════╝ ╚═╝     ╚═╝╚═╝     ╚═╝╚══════╝╚══════╝╚═╝  ╚═╝                          
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

# TODO: Import all misc training data sources and merge with site summaries from QBERD and QBEIS into TD_SITE_POOL


#   ______    ____        _______                             __           __         __         
#  |__    |  |_   |      |_     _|.--------.-----.-----.----.|  |_     .--|  |.---.-.|  |_.---.-.
#  |__    |__ _|  |_      _|   |_ |        |  _  |  _  |   _||   _|    |  _  ||  _  ||   _|  _  |
#  |______|__|______|    |_______||__|__|__|   __|_____|__|  |____|    |_____||___._||____|___._|
#                                          |__|                                                  
### TODO: Compile sites from various sources (SBC_TD_pool.csv)
# 1.4) Import training data sources
# qberd <- read.csv("training_data_inputs/QBERD_TEST.csv")
# qbeis <- read.csv("training_data_inputs/QBEIS_TEST.csv")
# rapid <- read.csv("training_data_inputs/RAPID_TEST.csv")
# tern <- read.csv("training_data_inputs/TERN_TEST.csv")
# bcc <- read.csv("training_data_inputs/BCC_TEST.csv")
# qval <- read.csv("training_data_inputs/QVAL_TEST.csv")
# quat <- read.csv("training_data_inputs/QUAT_TEST.csv")
# desktop <- read.csv("training_data_inputs/DESKTOP_TEST.csv")
# inferred <- read.csv("training_data_inputs/INFERRED_TEST.csv")


# poi <- read.csv("training_data_inputs/REv13_POI_TEST.csv") # Use this large dummy POI dataset for testing

##### 4) Regional Ecosystem Assignment #####                                                                                                                                                           
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                   ██╗  ██╗       ██████╗ ███████╗     █████╗ ███████╗███████╗██╗ ██████╗ ███╗   ██╗███╗   ███╗███████╗███╗   ██╗████████╗                     
#                   ██║  ██║       ██╔══██╗██╔════╝    ██╔══██╗██╔════╝██╔════╝██║██╔════╝ ████╗  ██║████╗ ████║██╔════╝████╗  ██║╚══██╔══╝                     
#                   ███████║       ██████╔╝█████╗      ███████║███████╗███████╗██║██║  ███╗██╔██╗ ██║██╔████╔██║█████╗  ██╔██╗ ██║   ██║                        
#                   ╚════██║       ██╔══██╗██╔══╝      ██╔══██║╚════██║╚════██║██║██║   ██║██║╚██╗██║██║╚██╔╝██║██╔══╝  ██║╚██╗██║   ██║                        
#                        ██║██╗    ██║  ██║███████╗    ██║  ██║███████║███████║██║╚██████╔╝██║ ╚████║██║ ╚═╝ ██║███████╗██║ ╚████║   ██║                        
#                        ╚═╝╚═╝    ╚═╝  ╚═╝╚══════╝    ╚═╝  ╚═╝╚══════╝╚══════╝╚═╝ ╚═════╝ ╚═╝  ╚═══╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═══╝   ╚═╝                        
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝ 


# TODO: Check REs from TD_SITE_POOL against latest version of Preclear RE mapping (RE1-RE5) to see if mapped and current. Use REDD and versioning to reassign
# TODO: Develop workflow to assign RE to sites where this data is missing or above doesn't work based on spatial intersect, landzone filtering and then match composition to remaining pool of REs

# FROM training_data_deficit_v1.0 -> calculates site thresholds based on latest version of RE mapping
# 3.1) Import data
# TODO: Injest and process raw gdb and calculate area in R to replace manual QGIS step - read then write as 2d gpkg to be imported?
preclear <- read.csv("tabular_inputs/PC_RE_v14_3577_aream2_SBCfiltered.csv") # done in QGIS due to Z dimension in RE mapping? Layer from gdb, exported as gpkg 3577, then $area - area_m2, export csv
redd <- read.csv("tabular_inputs/REDD_v13.1_2024.csv")
re <- read.csv("tabular_inputs/regional_ecosystem_2024.csv")

# 3.2) Calculate preclear hectare summary and number of reference sites required
preclear_summary <- preclear %>%
  group_by(RE1) %>%
  summarise(
    CUMULATIVE_AREA_M2 = sum(area_m2, na.rm = TRUE),
    CUMULATIVE_AREA_HA = CUMULATIVE_AREA_M2 / 10000
  ) %>%
  mutate(
    REF_SITES_REQUIRED = case_when(
      CUMULATIVE_AREA_HA <= 400 ~ 1,
      CUMULATIVE_AREA_HA <= 800 ~ 2,
      CUMULATIVE_AREA_HA <= 1200 ~ 3,
      CUMULATIVE_AREA_HA <= 1600 ~ 4,
      CUMULATIVE_AREA_HA > 1600 ~ 5
    ),
    AREA_THRESHOLD = case_when(
      CUMULATIVE_AREA_HA <= 400 ~ "<=400 ha",
      CUMULATIVE_AREA_HA <= 800 ~ "401–800 ha",
      CUMULATIVE_AREA_HA <= 1200 ~ "801–1200 ha",
      CUMULATIVE_AREA_HA <= 1600 ~ "1201–1600 ha",
      CUMULATIVE_AREA_HA > 1600 ~ ">1600 ha"
    )
  ) %>%
  select(-CUMULATIVE_AREA_M2)

# 3.3) Join in relevant columns from REDD and RE
preclear_summary <- preclear_summary %>%
  left_join(
    re %>% select(
      NAME,
      DESCRIPTION,
      SHORT_DESCRIPTION,
      STRUCTURE_CODE,
      BVG1M,
      HABITAT,
      SOIL,
      GEOLOGY,
      EXTENT_WITHIN_PROTECTED_AREAS,
      PROTECTED_AREAS
    ),
    by = c("RE1" = "NAME")
  )

# 3.4) Write and clean up
write.csv(preclear_summary, "tabular_inputs/SBC_td_deficit_REv14.csv", row.names = FALSE)




##### 5) Site scoring #####  
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#                          ███████╗       ███████╗██╗████████╗███████╗    ███████╗ ██████╗ ██████╗ ██████╗ ██╗███╗   ██╗ ██████╗                               
#                          ██╔════╝       ██╔════╝██║╚══██╔══╝██╔════╝    ██╔════╝██╔════╝██╔═══██╗██╔══██╗██║████╗  ██║██╔════╝                               
#                          ███████╗       ███████╗██║   ██║   █████╗      ███████╗██║     ██║   ██║██████╔╝██║██╔██╗ ██║██║  ███╗                              
#                          ╚════██║       ╚════██║██║   ██║   ██╔══╝      ╚════██║██║     ██║   ██║██╔══██╗██║██║╚██╗██║██║   ██║                              
#                          ███████║██╗    ███████║██║   ██║   ███████╗    ███████║╚██████╗╚██████╔╝██║  ██║██║██║ ╚████║╚██████╔╝                              
#                          ╚══════╝╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚══════╝ ╚═════╝ ╚═════╝ ╚═╝  ╚═╝╚═╝╚═╝  ╚═══╝ ╚═════╝                               
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

# TODO: Incorporate threshold and continous scoring scripts to score sites with BioCondition attributes
# This script is dependent on QBERD, QBEIS and RAPID site summaries and scoring databricks workflows. 
# Other misc training sources are stored locally and are not dynamic




##### 6) Site assessment #####  
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝
#               ██████╗        ███████╗██╗████████╗███████╗     █████╗ ███████╗███████╗███████╗███████╗███████╗███╗   ███╗███████╗███╗   ██╗████████╗
#              ██╔════╝        ██╔════╝██║╚══██╔══╝██╔════╝    ██╔══██╗██╔════╝██╔════╝██╔════╝██╔════╝██╔════╝████╗ ████║██╔════╝████╗  ██║╚══██╔══╝
#              ███████╗        ███████╗██║   ██║   █████╗      ███████║███████╗███████╗█████╗  ███████╗███████╗██╔████╔██║█████╗  ██╔██╗ ██║   ██║   
#              ██╔═══██╗       ╚════██║██║   ██║   ██╔══╝      ██╔══██║╚════██║╚════██║██╔══╝  ╚════██║╚════██║██║╚██╔╝██║██╔══╝  ██║╚██╗██║   ██║   
#              ╚██████╔╝██╗    ███████║██║   ██║   ███████╗    ██║  ██║███████║███████║███████╗███████║███████║██║ ╚═╝ ██║███████╗██║ ╚████║   ██║   
#               ╚═════╝ ╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚═╝  ╚═╝╚══════╝╚══════╝╚══════╝╚══════╝╚══════╝╚═╝     ╚═╝╚══════╝╚═╝  ╚═══╝   ╚═╝   
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝                                                                                                                                                 
#   ______    ____        _______                             __           __         __         
#  |    __|  |_   |      |_     _|.--------.-----.-----.----.|  |_     .--|  |.---.-.|  |_.---.-.
#  |  __  |__ _|  |_      _|   |_ |        |  _  |  _  |   _||   _|    |  _  ||  _  ||   _|  _  |
#  |______|__|______|    |_______||__|__|__|   __|_____|__|  |____|    |_____||___._||____|___._|
#                                          |__|                                                  
poi <- read.csv("training_data_inputs/REv13_POI_TEST.csv") # Use this large dummy POI dataset for testing
redd <- read.csv("tabular_inputs/REDD_v13.1_2024.csv")
re <- read.csv("tabular_inputs/regional_ecosystem_2024.csv")
#   ______    ______      _______         __           __                                                           __              
#  |    __|  |__    |    |_     _|.---.-.|  |--.--.--.|  |.---.-.----.    .-----.----.-----.----.-----.-----.-----.|__|.-----.-----.
#  |  __  |__|    __|      |   |  |  _  ||  _  |  |  ||  ||  _  |   _|    |  _  |   _|  _  |  __|  -__|__ --|__ --||  ||     |  _  |
#  |______|__|______|      |___|  |___._||_____|_____||__||___._|__|      |   __|__| |_____|____|_____|_____|_____||__||__|__|___  |
#                                                                         |__|                                               |_____|
# 6.2.1) Add structure code to complied TD 
poi <- poi %>%
  left_join(re %>% select (NAME, STRUCTURE_CODE),
            by = c("RE1" = "NAME")) %>%
  relocate(STRUCTURE_CODE, .after = DBVG1M)

# 6.2.2) Add fire guidelines to complied TD
poi <- poi %>%
  mutate(RE1_modified = sub("^(\\d+\\.\\d+\\.\\d+).*", "\\1", RE1)) %>%
  left_join(redd %>% select(NAME, FIRE_GUIDELINES),
            by = c("RE1_modified" = "NAME")) %>%
  select(-RE1_modified) %>% 
  relocate(FIRE_GUIDELINES, .after = COLLECTION_NOTES)

# 6.2.3) Add GDA94, GDA2020 coordinates
# Define a function that processes poi and returns it with new columns
add_gda_coords <- function(df, x_col = "x_3577", y_col = "y_3577") {
  # Convert to sf
  df_sf <- st_as_sf(df, coords = c(x_col, y_col), crs = 3577)
  
  # Extract GDA94 coords
  gda94 <- st_transform(df_sf, 4283) %>%
    mutate(
      x_gda94 = st_coordinates(.)[, 1],
      y_gda94 = st_coordinates(.)[, 2]
    ) %>%
    st_drop_geometry() %>%
    select(x_gda94, y_gda94)
  
  # Extract GDA2020 coords
  gda2020 <- st_transform(df_sf, 7844) %>%
    mutate(
      x_gda2020 = st_coordinates(.)[, 1],
      y_gda2020 = st_coordinates(.)[, 2]
    ) %>%
    st_drop_geometry() %>%
    select(x_gda2020, y_gda2020)
  
  # Bind new columns and relocate
  df <- df %>%
    bind_cols(gda94) %>%
    bind_cols(gda2020) %>%
    relocate(x_gda94, y_gda94, x_gda2020, y_gda2020, .after = ACCURACY)
  
  return(df)
}

# Apply the function (no intermediates left behind)
poi <- add_gda_coords(poi)

# 6.2.4) Create sf and spatial processing objects
# Ensure poi is in the same CRS as the vector data
poi_sf <- st_as_sf(poi, coords = c("x_3577", "y_3577"), crs = 3577)  ### MAY HAVE TO SWAP TO 3577 

# Convert to terra vect for efficient extraction
poi_vect <- vect(poi_sf)

# Set chunk size (adjustable)
chunk_size <- 50000  
n_chunks <- ceiling(nrow(poi_vect) / chunk_size)


#   ______    ______      _______               __   __         __                                               __              
#  |    __|  |__    |    |     __|.-----.---.-.|  |_|__|.---.-.|  |    .-----.----.-----.----.-----.-----.-----.|__|.-----.-----.
#  |  __  |__|__    |    |__     ||  _  |  _  ||   _|  ||  _  ||  |    |  _  |   _|  _  |  __|  -__|__ --|__ --||  ||     |  _  |
#  |______|__|______|    |_______||   __|___._||____|__||___._||__|    |   __|__| |_____|____|_____|_____|_____||__||__|__|___  |
#                                 |__|                                 |__|                                               |_____|

### 6.3.1) Regional Ecosystem Mapping
# 6.3.1.1) Import most recent RE gdb series  .....................................................................................................................


# 6.3.1.2) Sample the gdb in chunks ..............................................................................................................................


# ###QCHAT SOLUTION - UNTESTED!
# 
# library(sf)
# library(terra)
# library(dplyr)
# 
# # Path to the .gdb file
# gdb_path <- "path_to_your_file.gdb"
# 
# # List all layers in the .gdb file
# gdb_layers <- st_layers(gdb_path)$name
# print(gdb_layers)  # Inspect available layers
# 
# # Convert poi_sf to terra vector for efficient processing
# poi_vect <- vect(poi_sf)
# 
# # Set chunk size
# chunk_size <- 50000
# n_chunks <- ceiling(nrow(poi_vect) / chunk_size)
# 
# # Loop through each layer in the .gdb
# for (layer_name in gdb_layers) {
#   message("Processing layer: ", layer_name)
#   
#   # Load the current polygon layer
#   polygon_layer <- st_read(gdb_path, layer = layer_name)
#   
#   # Skip non-polygon layers (if any)
#   if (!inherits(polygon_layer, "sf") || !any(st_geometry_type(polygon_layer) %in% c("POLYGON", "MULTIPOLYGON"))) {
#     message("Skipping non-polygon layer: ", layer_name)
#     next
#   }
#   
#   # Convert polygon layer to terra format
#   polygon_vect <- vect(polygon_layer)
#   
#   # Initialise a list to store results for this layer
#   results_list <- vector("list", n_chunks)
#   
#   # Process poi_sf in chunks
#   for (i in seq_len(n_chunks)) {
#     start_idx <- (i - 1) * chunk_size + 1
#     end_idx <- min(i * chunk_size, nrow(poi_vect))
#     
#     # Extract chunk of points
#     chunk <- poi_vect[start_idx:end_idx, ]
#     
#     # Perform spatial join (intersect points with polygons)
#     joined <- terra::intersect(chunk, polygon_vect)
#     
#     # Extract relevant attributes from the polygon layer
#     if (!is.null(joined)) {
#       joined_df <- as.data.frame(joined)
#       results_list[[i]] <- joined_df
#     }
#     
#     # Clean up
#     rm(chunk, joined)
#     gc()
#     
#     message(sprintf("  Chunk %d/%d processed for layer: %s", i, n_chunks, layer_name))
#   }
#   
#   # Combine all results for this layer into a single data frame
#   results_df <- do.call(rbind, results_list)
#   
#   # Add results to poi_sf
#   if (!is.null(results_df) && nrow(results_df) > 0) {
#     col_prefix <- gsub("[^a-zA-Z0-9]", "_", layer_name)  # Sanitize layer name for column prefix
#     results_df <- results_df %>%
#       select(-geometry)  # Remove geometry column if present
#     colnames(results_df) <- paste0(col_prefix, "_", colnames(results_df))  # Prefix column names
#     
#     poi_sf <- poi_sf %>%
#       left_join(results_df, by = "fid")  # Replace with your unique ID column
#   }
#   
#   # Clean up after processing the layer
#   rm(polygon_layer, polygon_vect, results_list, results_df)
#   gc()
#   
#   message("Finished processing layer: ", layer_name)
# }
# 
# # Final clean-up
# rm(gdb_layers)
# gc()
# 
# 
# ################################################################################################################################################################
# 
# # 2.1.1) Create File Inventory
# # List all available years
# preclear_dirs <- dir_ls("spatial_inputs/Preclear", regexp = "Preclear_v\\d+-\\d+_\\d{4}")
# 
# # Extract years and paths
# preclear_years <- tibble(
#   path = preclear_dirs,
#   year = str_extract(path, "\\d{4}$")
# ) %>%
#   arrange(desc(year))  # Process newest first (often better quality)
# 
# # 2.1.2) Memory-efficient processing function
# process_preclear <- function(year_path, points_sf) {
#   # Read with geometry repair
#   layer <- st_read(
#     file.path(year_path, "data.gdb"),
#     quiet = TRUE,
#     stringsAsFactors = FALSE,
#     promote_to_multi = TRUE  # Force multi-geometries
#   ) %>% 
#     st_cast("MULTIPOLYGON") %>%  # Convert all to supported type
#     sf::st_make_valid() %>%  # More robust than sf::st_make_valid
#     st_simplify(preserveTopology = TRUE, dTolerance = 0.1) %>%  # Reduce complexity
#     select(any_of(c("re1", "dbvg1m")))
#   
#   # Skip if no valid geometries remain
#   if (nrow(layer) == 0 || all(is.na(st_dimension(layer)))) return(NULL)
#   
#   # Spatial join with error handling
#   result <- tryCatch({
#     st_join(
#       points_sf,
#       layer,
#       join = st_intersects,
#       left = TRUE,
#       largest = TRUE
#     ) %>%
#       st_drop_geometry() %>%
#       select(ends_with("re1"), ends_with("dbvg1m")) %>%
#       rename_with(~ paste0("preclear_", .x, "_", str_extract(year_path, "\\d{4}$")))
#   }, error = function(e) {
#     message("Failed on ", year_path, ": ", e$message)
#     NULL
#   })
#   
#   rm(layer); gc()
#   return(result)
# }
# 
# # Process sequentially (more reliable than parallel for problematic geometries)
# preclear_results <- map(
#   preclear_years$path, 
#   ~ process_preclear(., poi_sf),
#   .progress = TRUE
# ) %>% 
#   compact() %>%
#   reduce(left_join, by = "fid")  # Adjust 'fid' to your actual ID column
# 
# # 2.1.3) Merge final results
# poi_final <- poi_sf %>%
#   st_drop_geometry() %>%
#   left_join(preclear_results, by = "fid")
# 
# # Force final cleanup
# rm(preclear_results); gc()


# # Ensure poi is in the same CRS as the vector data
# poi_sf <- st_as_sf(poi, coords = c("x_gda94", "y_gda94"), crs = 4283) 
# 
# # Convert geometry and make valid
# pre_clear <- pre_clear %>%
#   st_cast("MULTIPOLYGON") %>%  # Convert geometry type
#   st_make_valid()  # Ensure validity
# 
# # Spatial join to extract attributes from 'preclear' at poi locations
# poi_sampled <- st_join(poi_sf, pre_clear, join = st_within)  # Use st_intersects if points can be on boundaries



#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝


### 6.3.2) High Value Regrowth Mapping
# 6.3.2.1) Import HVR gdb series  ..............................................................................................................................


# 6.3.2.2) Sample the gdb in chunks ............................................................................................................................


#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 6.3.3) Land Use Mapping 
# 6.3.3.1) Import land use data series  ........................................................................................................................

# QLD LAND USE ALUM
# SLATS LANDCOVER
# LANDSAT LANDCOVER TEMPORAL?
# HUMAN MODIFICATION INDEX?


# 6.3.3.2) Sample the gdb in chunks ............................................................................................................................


#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝







# 2.3) Burn scars
# 2.3.1) Scrape and import burn scar rasters ...................................................................................................................

# Get list of all .tif files in Fire_scars folder
tif_files <- list.files(path = "spatial_inputs/burn_scars", pattern = "\\.tif$", full.names = TRUE, recursive = TRUE)

# 2.3.2) Sample the raster in chunks ...........................................................................................................................
# Process each raster file
for(tif_file in tif_files) {
  # Extract year from filename (assuming format like "firescar_2000.tif")
  year <- gsub(".*?(\\d{4}).*", "\\1", basename(tif_file))
  col_name <- paste0("FIRE_MONTH_", year)
  
  message("Processing: ", basename(tif_file), " (", col_name, ")")
  
  # Load raster
  r <- rast(tif_file)
  
  # Initialize result vector
  sampled_values <- rep(NA, nrow(poi_vect))
  
  # Process in chunks
  n_chunks <- ceiling(nrow(poi_vect) / chunk_size)
  
  for(i in seq_len(n_chunks)) {
    start_idx <- (i - 1) * chunk_size + 1
    end_idx <- min(i * chunk_size, nrow(poi_vect))
    
    chunk <- poi_vect[start_idx:end_idx]
    sampled_values[start_idx:end_idx] <- terra::extract(r, chunk)[, 2]
    
    rm(chunk)
    gc()
    
    message(sprintf("  Chunk %d/%d processed", i, n_chunks))
  }
  
  # Add results to poi_sf
  poi_sf[[col_name]] <- sampled_values
  
  # Clean up
  rm(r, sampled_values)
  gc()
}

rm(tif_file, tif_files)
gc()

# Save intermediate fire sampled sites
st_write(poi_sf, file.path(int_dir, "fire_poi_sampled.gpkg"), quiet = TRUE, delete_dsn = TRUE)

# 2.3.3) Add burn scar flag ....................................................................................................................................


##### TODO:
#   ██████╗██╗  ██╗███████╗ ██████╗██╗  ██╗    ███████╗██╗      █████╗  ██████╗     ██╗      ██████╗  ██████╗ ██╗ ██████╗
#  ██╔════╝██║  ██║██╔════╝██╔════╝██║ ██╔╝    ██╔════╝██║     ██╔══██╗██╔════╝     ██║     ██╔═══██╗██╔════╝ ██║██╔════╝
#  ██║     ███████║█████╗  ██║     █████╔╝     █████╗  ██║     ███████║██║  ███╗    ██║     ██║   ██║██║  ███╗██║██║     
#  ██║     ██╔══██║██╔══╝  ██║     ██╔═██╗     ██╔══╝  ██║     ██╔══██║██║   ██║    ██║     ██║   ██║██║   ██║██║██║     
#  ╚██████╗██║  ██║███████╗╚██████╗██║  ██╗    ██║     ███████╗██║  ██║╚██████╔╝    ███████╗╚██████╔╝╚██████╔╝██║╚██████╗
#   ╚═════╝╚═╝  ╚═╝╚══════╝ ╚═════╝╚═╝  ╚═╝    ╚═╝     ╚══════╝╚═╝  ╚═╝ ╚═════╝     ╚══════╝ ╚═════╝  ╚═════╝ ╚═╝ ╚═════╝
#                                                                                                                        

# CAN IMPORT FIRE SAMPLED POI HERE TO SKIP PROCESSING
poi_sf <- st_read("intermediates/fire_poi_sampled.gpkg", quiet = TRUE) # Load pre-sampled fire data

# Fire columns in ascending order by year
col_names <- colnames(poi_sf)
firemonth_cols <- grep("^FIRE_MONTH_", col_names, value = TRUE)
sorted_firemonth_cols <- firemonth_cols[order(as.numeric(sub("FIRE_MONTH_", "", firemonth_cols)))]
reordered_cols <- c(setdiff(col_names, firemonth_cols), sorted_firemonth_cols)
poi_sf <- poi_sf[, reordered_cols]

# Vectorised function to process fire metrics
process_fire_metrics <- function(poi_sf, start_year = 1987) {
  # Drop geometry and extract FIRE_MONTH columns as numeric matrix
  fire_cols <- poi_sf %>%
    st_drop_geometry() %>%
    select(starts_with("FIRE_MONTH_")) %>%
    mutate(across(everything(), as.numeric))
  
  fire_months_matrix <- as.matrix(fire_cols)
  fire_years <- as.numeric(str_extract(colnames(fire_cols), "\\d{4}"))
  n <- nrow(fire_months_matrix)
  
  # Mask out invalid months (not 1–12)
  valid_mask <- fire_months_matrix >= 1 & fire_months_matrix <= 12
  valid_fire_months <- ifelse(valid_mask, fire_months_matrix, NA)
  
  # Compute most recent burn per row using matrix indexing
  most_recent_idx <- max.col(!is.na(valid_fire_months), ties.method = "last")
  has_burn <- rowSums(!is.na(valid_fire_months)) > 0
  
  # Preallocate and fill most recent burn dates
  most_recent_burn <- rep(NA, n)
  most_recent_burn[has_burn] <- as.Date(sprintf(
    "%04d-%02d-01",
    fire_years[most_recent_idx[has_burn]],
    valid_fire_months[cbind(which(has_burn), most_recent_idx[has_burn])]
  ))
  
  # Calculate fire indices
  fire_indices <- sweep(valid_fire_months, 2, (fire_years - start_year) * 12, `+`)
  
  # Mean fire return interval (vectorized alternative to complex apply)
  fire_intervals <- lapply(1:n, function(i) {
    months <- valid_fire_months[i, ]
    idxs <- which(!is.na(months))
    if (length(idxs) > 1) {
      dates <- as.Date(sprintf("%04d-%02d-01", fire_years[idxs], months[idxs]))
      dates <- sort(dates)
      return(diff(as.numeric(format(dates, "%Y")) * 12 + as.numeric(format(dates, "%m"))))
    } else {
      return(NA)
    }
  })
  
  fire_return_interval <- vapply(fire_intervals, function(x) {
    if (is.numeric(x)) mean(x, na.rm = TRUE) else NA_real_
  }, numeric(1))
  
  # Vectorized collection date processing
  collection_date <- dmy(poi_sf$COLLECTION_DATE)
  collection_index <- (year(collection_date) - start_year) * 12 + month(collection_date)
  
  # Burns post collection: count fire indices greater than collection index
  burns_after_collection <- rowSums(sweep(fire_indices, 1, collection_index, FUN = `>`) & !is.na(fire_indices), na.rm = TRUE)
  
  # Final mutation
  poi_sf <- poi_sf %>%
    mutate(
      BURNS_1987_2024 = rowSums(!is.na(valid_fire_months), na.rm = TRUE),
      fire_intervals = fire_intervals,
      MEAN_FIRE_RETURN_INTERVAL = fire_return_interval,
      collection_date = collection_date,
      collection_index = collection_index,
      BURNS_POST_COLLECTION = burns_after_collection,
      MOST_RECENT_BURN = as.Date(most_recent_burn, origin = "1970-01-01"),
      MIN_FIRE_INTERVAL = as.numeric(str_extract(FIRE_GUIDELINES, "(?<=INTERVAL_MIN: )\\d+"))
    )
  
  return(poi_sf)
}

# Vectorised calculation of fire disturbance flags
calculate_fd_flags <- function(poi_sf, fixed_dates) {
  # Vectorized collection date
  collection_date <- dmy(poi_sf$COLLECTION_DATE)
  
  # Time since most recent fire (in months)
  time_since_fire <- ifelse(
    !is.na(poi_sf$MOST_RECENT_BURN),
    (year(collection_date) - year(poi_sf$MOST_RECENT_BURN)) * 12 +
      (month(collection_date) - month(poi_sf$MOST_RECENT_BURN)),
    NA
  )
  
  # Extract key columns as vectors for speed
  most_recent_burn <- poi_sf$MOST_RECENT_BURN
  min_fire_interval_months <- poi_sf$MIN_FIRE_INTERVAL * 12
  n <- nrow(poi_sf)
  
  # Preallocate FD_FLAG matrix
  years <- year(fixed_dates)
  FD_FLAGS <- matrix("Unburnt", nrow = n, ncol = length(fixed_dates))
  FD_3Y <- matrix("Unburnt in assessment period or three years prior", nrow = n, ncol = length(fixed_dates))
  
  for (j in seq_along(fixed_dates)) {
    fixed_date <- fixed_dates[j]
    
    time_to_fixed <- ifelse(
      !is.na(most_recent_burn),
      (year(fixed_date) - year(most_recent_burn)) * 12 +
        (month(fixed_date) - month(most_recent_burn)),
      NA
    )
    
    fire_after_collection <- !is.na(most_recent_burn) & (most_recent_burn > collection_date)
    
    # Conditions for flags
    recovered <- fire_after_collection & !is.na(time_to_fixed) & (time_to_fixed > min_fire_interval_months)
    recovering <- fire_after_collection & !is.na(time_to_fixed) & (time_to_fixed <= min_fire_interval_months)
    
    FD_FLAGS[recovered, j] <- "Recovered"
    FD_FLAGS[recovering, j] <- "Recovering"
    
    # 3-year burn window
    window_start <- as.Date(paste0(year(fixed_date) - 3, "-01-01"))
    window_end <- as.Date(paste0(year(fixed_date), "-12-31"))
    in_window <- !is.na(most_recent_burn) & most_recent_burn >= window_start & most_recent_burn <= window_end
    
    FD_3Y[in_window, j] <- "Burnt in assessment period or three years prior"
  }
  
  # Add column names
  colnames(FD_FLAGS) <- paste0("FD_FLAG_", years)
  colnames(FD_3Y) <- paste0("FD_", years, "_3Y")
  
  # Combine with original data
  poi_sf <- poi_sf %>%
    mutate(time_since_fire = time_since_fire) %>%
    bind_cols(as_tibble(FD_FLAGS)) %>%
    bind_cols(as_tibble(FD_3Y))
  
  return(poi_sf)
}

# Process fire data, calculate metrics and fire disturbance flags
poi_sf <- process_fire_metrics(poi_sf)
fixed_dates <- as.Date(c("2017-01-01", "2019-01-01", "2021-01-01", "2023-01-01"))
poi_sf <- calculate_fd_flags(poi_sf, fixed_dates)

# Drop intermediate variables and clean up (optional) 
poi_sf <- poi_sf %>%
  select(-FIRE_GUIDELINES, -fire_intervals, -collection_date, -collection_index, -all_of(firemonth_cols))
rm(col_names, firemonth_cols, fixed_dates, reordered_cols, sorted_firemonth_cols, process_fire_metrics, calculate_fd_flags)
gc()

#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 2.4) Age Since Woody Disturbance
# 2.4.1) Import woody disturbance product  .....................................................................................................................
slats_disturbance <- terra::rast("spatial_inputs/SLATS_2023/woody_disturbance/DP_QLD_WOODY_AGE_2022_COG.tif") # time since clearing

# 2.4.2) Sample the raster in chunks ...........................................................................................................................
# Initialize result vector
sampled_values <- rep(NA, nrow(poi_vect))

# Process in chunks
for (i in seq_len(n_chunks)) {
  start_idx <- (i - 1) * chunk_size + 1
  end_idx <- min(i * chunk_size, nrow(poi_vect))
  chunk <- poi_vect[start_idx:end_idx]
  sampled_values[start_idx:end_idx] <- terra::extract(slats_disturbance, chunk)[, 2]
  rm(chunk)
  gc()
}

# Add results back to your sf object
poi_sf$ASWD_2022 <- sampled_values

# 2.4.3) Add in disturbance flag
# Initialize new columns with NA
poi_sf$COLLECTION_YEAR <- NA_integer_
poi_sf$WD_YEAR <- NA_integer_
poi_sf$WD_TIMING <- NA_character_

# Process in chunks
for (i in seq_len(n_chunks)) {
  start_idx <- (i - 1) * chunk_size + 1
  end_idx <- min(i * chunk_size, nrow(poi_sf))
  chunk <- poi_sf[start_idx:end_idx, ]
  chunk$COLLECTION_YEAR <- as.numeric(format(
    as.Date(chunk$COLLECTION_DATE, format = "%d/%m/%Y"), 
    "%Y"))
  # Identify rows to process (sampled_value between 1-32)
  process_rows <- which(chunk$ASWD_2022 >= 1 & chunk$ASWD_2022 <= 32)
  if (length(process_rows) > 0) {
    # Calculate WD_YEAR only for valid rows
    chunk$WD_YEAR[process_rows] <- 2022 - chunk$ASWD_2022[process_rows]
    # Determine WD timing for valid rows
    chunk$WD_TIMING[process_rows] <- case_when(
      chunk$WD_YEAR[process_rows] < chunk$COLLECTION_YEAR[process_rows] ~ "Before collection",
      chunk$WD_YEAR[process_rows] == chunk$COLLECTION_YEAR[process_rows] ~ "Same year",
      chunk$WD_YEAR[process_rows] > chunk$COLLECTION_YEAR[process_rows] ~ "After collection"
    )
  }
  
  # Write results back to main sf object
  poi_sf$COLLECTION_YEAR[start_idx:end_idx] <- chunk$COLLECTION_YEAR
  poi_sf$WD_YEAR[start_idx:end_idx] <- chunk$WD_YEAR
  poi_sf$WD_TIMING[start_idx:end_idx] <- chunk$WD_TIMING
  
  # Clean up
  rm(chunk)
  gc()
  
  message(sprintf("Processed chunk %d of %d (rows %d to %d) - %d valid rows", 
                  i, n_chunks, start_idx, end_idx, length(process_rows)))
}

# clean up
poi_sf <- poi_sf %>%
  select(-COLLECTION_YEAR, -WD_YEAR)
rm(slats_disturbance, i, end_idx, process_rows, start_idx, sampled_values)

#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝

### 2.5) SLATS Change of Woody Vegetation
# 2.5.1) Import SLATS gdb series  ..............................................................................................................................


# 2.5.2) Sample the gdb in chunks ..............................................................................................................................


#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝




##### 7) Site auditing ##### 
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝  
#                              ███████╗     ███████╗██╗████████╗███████╗     █████╗ ██╗   ██╗██████╗ ██╗████████╗██╗███╗   ██╗ ██████╗ 
#                              ╚════██║     ██╔════╝██║╚══██╔══╝██╔════╝    ██╔══██╗██║   ██║██╔══██╗██║╚══██╔══╝██║████╗  ██║██╔════╝ 
#                                  ██╔╝     ███████╗██║   ██║   █████╗      ███████║██║   ██║██║  ██║██║   ██║   ██║██╔██╗ ██║██║  ███╗
#                                 ██╔╝      ╚════██║██║   ██║   ██╔══╝      ██╔══██║██║   ██║██║  ██║██║   ██║   ██║██║╚██╗██║██║   ██║
#                                 ██║██╗    ███████║██║   ██║   ███████╗    ██║  ██║╚██████╔╝██████╔╝██║   ██║   ██║██║ ╚████║╚██████╔╝
#                                 ╚═╝╚═╝    ╚══════╝╚═╝   ╚═╝   ╚══════╝    ╚═╝  ╚═╝ ╚═════╝ ╚═════╝ ╚═╝   ╚═╝   ╚═╝╚═╝  ╚═══╝ ╚═════╝ 
#  ██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗██╗
#  ╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝╚═╝  

# TODO:
# check for any evidence of disturbance post collection
# Determine if unscored sites can be considered in reference condition, and assign -1
# Generate low, mid and high strict audits

# cross reference discarded site REs with TD deficits and analogous to reprioritise












































#------------------------------------------ 2) Training Data Compiler -------------------------------------------------#
# 2.1) Mutate and join training data sources

# 2.2) Remove duplicates and ensure most recent site visit

# 2.3) Clean dataframe

# 2.4) Spatial intersect of RE and flag valid/mismatch RE

# 2.5) Create date + accuracy cuts of dataset to run through spatial auditing
# past was limited to 1995, but look at using reference condition if in PA and no disturbance
# pass accuracy was 200 m but explore higher threshold to see if anything else of value appears
# 

#------------------------------------------ 3) Training Data Compiler -------------------------------------------------#
# 3.1) Flag proximity trigger for roads, re boundary, remnant, hvr, other td points
# create 90 m buffer around all points using sentinel grid window 9x9
# intersect point assessment window with trigger layers
# look at eight surrounding 9x9 assessment windows and check triggers to suggest point relocation